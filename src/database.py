import pandas as pd
import os
import json
from pathlib import Path
from datetime import datetime, timedelta
import streamlit as st
from huggingface_hub import HfApi
from src.config import DATA_DIR, HF_TOKEN, HF_DATASET_ID

class DataRepository:
    """
    [Metadata-Driven Data Repository]
    - íŒŒì¼ ìˆ˜ì • ì‹œê°„(OS Time) ëŒ€ì‹ , ë³„ë„ì˜ ë©”íƒ€ë°ì´í„°(Last Checked)ë¡œ ê°±ì‹  ì£¼ê¸°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
    - ë°œí‘œ ì£¼ê¸°ê°€ ê¸´ ë°ì´í„°(ì›”ê°„/ë¶„ê¸°)ì˜ ë¶ˆí•„ìš”í•œ API í˜¸ì¶œì„ ì›ì²œ ì°¨ë‹¨í•©ë‹ˆë‹¤.
    """
    def __init__(self):
        self.data_dir = Path(DATA_DIR)
        if not self.data_dir.exists():
            self.data_dir.mkdir(parents=True, exist_ok=True)
            
        self.api = HfApi(token=HF_TOKEN)
        self.repo_id = HF_DATASET_ID

    def get_data(self, filename: str, loader, check_interval_days: int = 0.00035, start_date=None, **kwargs) -> pd.DataFrame:
        """
        :param check_interval_days: ì´ ê¸°ê°„ ë‚´ì—ëŠ” ì¬ì¡°íšŒë¥¼ ì‹œë„í•˜ì§€ ì•ŠìŒ (ë°œí‘œ ì£¼ê¸° ê³ ë ¤)
        """
        file_path = self.data_dir / filename
        meta_path = self.data_dir / f"{filename}.meta.json"
        
        # 1. íŒŒì¼ì´ ì—†ìœ¼ë©´ -> ë¬´ì¡°ê±´ ì‹ ê·œ ìƒì„± (HF ë³µêµ¬ ì‹œë„ í¬í•¨)
        if not file_path.exists():
            if self._pull_from_hub(filename): # ë°ì´í„° ë³µêµ¬
                self._pull_from_hub(f"{filename}.meta.json") # ë©”íƒ€ë°ì´í„°ë„ ê°™ì´ ë³µêµ¬
            else:
                return self._fetch_and_save(filename, loader, meta_path, start_date=start_date, **kwargs)

        # 2. íŒŒì¼ ë¡œë“œ
        df_existing = self._load_csv(file_path)
        if df_existing.empty:
             # ë¹ˆ ê»ë°ê¸°ë§Œ ìˆìœ¼ë©´ ë‹¤ì‹œ ë°›ìŒ
             return self._fetch_and_save(filename, loader, meta_path, start_date=start_date, **kwargs)

        # 3. [í•µì‹¬] ì¿¨íƒ€ì„(Last Checked) í™•ì¸
        last_checked = self._get_last_checked(meta_path)
        
        # ì•„ì§ ì¿¨íƒ€ì„ ì•ˆ ì§€ë‚¬ìœ¼ë©´ -> ê¸°ì¡´ ë°ì´í„° ë¦¬í„´ (ê²°ì¸¡ì¹˜ê°€ ìˆë“  ë§ë“  ì‹ ê²½ ë„ê³  ë¦¬í„´)
        if datetime.now() - last_checked < timedelta(days=check_interval_days):
            # print(f"zzz [Repo] ì¿¨íƒ€ì„ ì¤‘: {filename} (ë‚¨ì€ ì‹œê°„: {timedelta(days=check_interval_days) - (datetime.now() - last_checked)})")
            return df_existing

        # -----------------------------------------------------------
        # ì—¬ê¸° ë‚´ë ¤ì™”ë‹¤ëŠ” ê±´ ì¿¨íƒ€ì„ì´ ëë‚¬ë‹¤ëŠ” ëœ» -> ì´ì œì•¼ API í˜¸ì¶œ ì‹œë„
        # -----------------------------------------------------------
        
        try:
            current_max_date = df_existing['date'].max() if 'date' in df_existing.columns else None
            today = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)

            # A. ê³¼ê±° ë°ì´í„° êµ¬ë© ë©”ìš°ê¸° (ì´ê±´ ì¿¨íƒ€ì„ ì§€ë‚¬ìœ¼ë‹ˆ í•œë²ˆ ì²´í¬)
            if start_date:
                current_min_date = df_existing['date'].min() if 'date' in df_existing.columns else None
                req_start = pd.to_datetime(start_date).replace(tzinfo=None)
                if current_min_date and current_min_date > req_start + timedelta(days=5):
                    print(f"ğŸ”„ [Repo] ê³¼ê±° ë°ì´í„° ë¶€ì¡± ë°œê²¬ -> ì „ì²´ ì¬ìˆ˜ì§‘")
                    return self._fetch_and_save(filename, loader, meta_path, start_date=start_date, **kwargs)

            # B. ìµœì‹  ë°ì´í„° ì´ì–´ë¶™ì´ê¸°
            # (ë°ì´í„°ê°€ ì˜¤ëŠ˜ë³´ë‹¤ ì˜›ë‚  ê²ƒì„ -> ìƒˆ ë°ì´í„°ê°€ ë‚˜ì™”ë‚˜ í™•ì¸í•´ë³¼ ì‹œê°„ì„)
            if current_max_date and current_max_date < today - timedelta(days=1):
                next_day = current_max_date + timedelta(days=1)
                
                # ì•„ì§ ë¯¸ë˜ ë‚ ì§œë©´ íŒ¨ìŠ¤ (ì´ëŸ´ ì¼ì€ ê±°ì˜ ì—†ì§€ë§Œ)
                if next_day > datetime.now():
                    self._update_meta(meta_path) # í™•ì¸í–ˆìŒì„ ê¸°ë¡
                    return df_existing

                print(f"ğŸ” [Repo] ê°±ì‹  ì£¼ê¸° ë„ë˜ ({check_interval_days}ì¼ ê²½ê³¼) -> API ì¡°íšŒ: {filename}")
                
                kwargs_copy = kwargs.copy()
                kwargs_copy.pop('start_date', None)
                
                df_new = loader.fetch_data(start_date=next_day.strftime('%Y-%m-%d'), **kwargs_copy)
                
                if not df_new.empty:
                    df_new = self._ensure_date_format(df_new)
                    
                    df_combined = pd.concat([df_existing, df_new])
                    df_combined = df_combined.drop_duplicates(subset=['date'], keep='last').sort_values('date')
                    
                    self._save_and_push(file_path, df_combined, filename, meta_path)
                    return df_combined
                else:
                    # **ì¤‘ìš”**: ë°ì´í„°ê°€ ì—†ì–´ë„(ë°œí‘œ ì•ˆ ë¨/íœ´ì¥) "í™•ì¸í–ˆìŒ"ì„ ê¸°ë¡í•´ì•¼ í•¨!
                    # ê·¸ë˜ì•¼ ë‚´ì¼ ë˜ í—›ë˜ì´ ì¡°íšŒí•˜ì§€ ì•Šê³  ì¿¨íƒ€ì„ì„ ê°€ì§.
                    print(f"ğŸ’¤ [Repo] ì‹ ê·œ ë°ì´í„° ì—†ìŒ (ë‹¤ìŒ ì£¼ê¸°ê¹Œì§€ ëŒ€ê¸°)")
                    self._update_meta(meta_path) # Last Checked ê°±ì‹ 
                    self._push_meta_only(filename, meta_path) # ë©”íƒ€íŒŒì¼ë§Œ ì—…ë¡œë“œ
                    return df_existing
            
            # C. ë°ì´í„°ê°€ ì´ë¯¸ ìµœì‹ ì„ (ì˜¤ëŠ˜ì ë°ì´í„°ê¹Œì§€ ìˆìŒ)
            else:
                self._update_meta(meta_path) # í™•ì¸í–ˆìŒ ê¸°ë¡
                return df_existing

        except Exception as e:
            print(f"âŒ [Repo] ìë™ ê°±ì‹  ì¤‘ ì˜¤ë¥˜ ({filename}): {e}")
            return df_existing

    # --- ë‚´ë¶€ ë©”ì„œë“œ ---

    def _fetch_and_save(self, filename, loader, meta_path, **kwargs) -> pd.DataFrame:
        try:
            df = loader.fetch_data(**kwargs)
            if df is not None and not df.empty:
                df = self._ensure_date_format(df)
                file_path = self.data_dir / filename
                self._save_and_push(file_path, df, filename, meta_path)
            return df
        except Exception:
            return pd.DataFrame()

    def _save_and_push(self, file_path, df, filename, meta_path):
        """ë°ì´í„° ì €ì¥ + ë©”íƒ€ë°ì´í„° ê°±ì‹  + ë‘˜ ë‹¤ ì—…ë¡œë“œ"""
        try:
            # 1. ë°ì´í„° ì €ì¥
            df.to_csv(file_path, index=False)
            
            # 2. ë©”íƒ€ë°ì´í„°(ì¡°íšŒ ì‹œê°) ê°±ì‹  ë° ì €ì¥
            self._update_meta(meta_path)
            
            # 3. HF ì—…ë¡œë“œ (ë°ì´í„° + ë©”íƒ€)
            try:
                self.api.upload_file(
                    path_or_fileobj=file_path, path_in_repo=f"data/{filename}",
                    repo_id=self.repo_id, repo_type="dataset"
                )
                self.api.upload_file(
                    path_or_fileobj=meta_path, path_in_repo=f"data/{filename}.meta.json",
                    repo_id=self.repo_id, repo_type="dataset"
                )
                # print(f"â˜ï¸ [Repo] ë™ê¸°í™” ì™„ë£Œ: {filename}")
            except Exception: pass
        except Exception: pass

    def _push_meta_only(self, filename, meta_path):
        """ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œë‘ê³  ë©”íƒ€ë°ì´í„°ë§Œ ì—…ë¡œë“œ (ì¡°íšŒ ê¸°ë¡ ê°±ì‹ ìš©)"""
        try:
            self.api.upload_file(
                path_or_fileobj=meta_path, path_in_repo=f"data/{filename}.meta.json",
                repo_id=self.repo_id, repo_type="dataset"
            )
        except Exception: pass

    def _update_meta(self, meta_path):
        """í˜„ì¬ ì‹œê°ì„ Last Checkedë¡œ ê¸°ë¡"""
        meta = {"last_checked": datetime.now().isoformat()}
        with open(meta_path, 'w') as f:
            json.dump(meta, f)

    def _get_last_checked(self, meta_path):
        """ë§ˆì§€ë§‰ ì¡°íšŒ ì‹œê° ë°˜í™˜ (ì—†ìœ¼ë©´ ì•„ì£¼ ì˜›ë‚ )"""
        if not meta_path.exists():
             return datetime.min
        try:
            with open(meta_path, 'r') as f:
                data = json.load(f)
                return datetime.fromisoformat(data['last_checked'])
        except:
            return datetime.min

    def _pull_from_hub(self, filename):
        try:
            self.api.hf_hub_download(repo_id=self.repo_id, filename=f"data/{filename}", repo_type="dataset", local_dir=DATA_DIR.parent)
            return True
        except: return False

    def _load_csv(self, file_path) -> pd.DataFrame:
        try:
            df = pd.read_csv(file_path)
            return self._ensure_date_format(df)
        except: return pd.DataFrame()

    def _ensure_date_format(self, df) -> pd.DataFrame:
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
            if df['date'].dt.tz is not None:
                df['date'] = df['date'].dt.tz_localize(None)
        return df